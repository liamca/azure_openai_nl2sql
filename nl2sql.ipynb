{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bab64bef-6e28-46e8-acc9-65c5aa1cb17f",
   "metadata": {},
   "source": [
    "# Python Configuration\n",
    "\n",
    "pip install pyodbc openai pandas ipython tabulate\n",
    "\n",
    "## MS SQL Server Library Config\n",
    "sudo apt-get update\n",
    "sudo ACCEPT_EULA=Y apt-get install -y msodbcsql18\n",
    "\n",
    "### optional: for bcp and sqlcmd\n",
    "sudo ACCEPT_EULA=Y apt-get install -y mssql-tools18\n",
    "echo 'export PATH=\"$PATH:/opt/mssql-tools18/bin\"' >> ~/.bashrc\n",
    "source ~/.bashrc\n",
    "\n",
    "### optional: for unixODBC development headers\n",
    "sudo apt-get install -y unixodbc-dev\n",
    "\n",
    "### optional: kerberos library for debian-slim distributions\n",
    "sudo apt-get install -y libgssapi-krb5-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dea43720-50fa-4ccc-9a38-ef3a2950f51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc  \n",
    "from openai import AzureOpenAI, OpenAIError  \n",
    "import openai  \n",
    "import json  \n",
    "import pickle  \n",
    "import random  \n",
    "import pandas as pd  \n",
    "from IPython.display import Markdown, display  \n",
    "import time   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d9a4f7c-4daf-42d3-8493-e50bd3d00370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration  \n",
    "openai_gpt_api_base =  \"https://<redacted>.openai.azure.com/\"\n",
    "openai_gpt_api_key =  \"<redacted>\"\n",
    "openai_gpt_api_version =   \"2024-02-15-preview\"\n",
    "openai_gpt_model =  \"gpt-4o\"\n",
    "openai_embedding_api_base = \"https://<redacted>.openai.azure.com/\"\n",
    "openai_embedding_api_key = \"<redacted>\"\n",
    "openai_embedding_api_version =  \"2024-02-15-preview\"\n",
    "openai_embedding_model = \"text-embedding-ada-002\"\n",
    "server = '<redacted>.database.windows.net'\n",
    "database = '<redacted>'\n",
    "username = '<redacted>'\n",
    "password = '<redacted>'\n",
    "\n",
    "openai_temperature = 0.0  \n",
    "max_tokens = 4096 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f9dab0f-41e1-4617-8d06-45f0abe8005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate table description using OpenAI  \n",
    "def generate_oai_table_desc(content, openai_gpt_api_base, openai_gpt_api_key, openai_gpt_api_version, openai_gpt_model):  \n",
    "    max_attempts = 6  \n",
    "    max_backoff = 60  \n",
    "    system_prompt = \"\"\"  \n",
    "    You are an intelligent assistant that helps create a description for a SQL Server table.   \n",
    "    You will be provided with details such as the table name, columns and column details.  \n",
    "    Do not create new column names or tables.  \n",
    "    Please output this description in markdown format.  \n",
    "    \"\"\"  \n",
    "    user_prompt = content  \n",
    "    gpt_client = AzureOpenAI(  \n",
    "        api_version=openai_gpt_api_version,  \n",
    "        azure_endpoint=openai_gpt_api_base,  \n",
    "        api_key=openai_gpt_api_key  \n",
    "    )  \n",
    "  \n",
    "    counter = 0  \n",
    "    incremental_backoff = 1  # seconds to wait on throttling - this will be incremental backoff  \n",
    "    while counter < max_attempts:  \n",
    "        try:  \n",
    "            response = gpt_client.chat.completions.create(  \n",
    "                model=openai_gpt_model,  \n",
    "                messages=[  \n",
    "                    {\"role\": \"system\", \"content\": system_prompt},  \n",
    "                    {\"role\": \"user\", \"content\": user_prompt}  \n",
    "                ],  \n",
    "                temperature=openai_temperature,  \n",
    "                max_tokens=max_tokens,  \n",
    "                top_p=0.95,  \n",
    "                frequency_penalty=0,  \n",
    "                presence_penalty=0,  \n",
    "                stop=None,  \n",
    "                stream=False  \n",
    "            )  \n",
    "            return response.choices[0].message.content  \n",
    "        except openai.APIError as ex:  \n",
    "            if str(ex.code) == \"429\":  \n",
    "                incremental_backoff = min(max_backoff, incremental_backoff * 1.5)  \n",
    "                print('Waiting to retry after', incremental_backoff, 'seconds...')  \n",
    "                counter += 1  \n",
    "                time.sleep(incremental_backoff)  \n",
    "            elif str(ex.code) == \"DeploymentNotFound\":  \n",
    "                print('Error: Deployment not found')  \n",
    "                return 'Error: Deployment not found'  \n",
    "            elif 'Error code: 40' in str(ex):  \n",
    "                print('Error: ' + str(ex))  \n",
    "                return 'Error:' + str(ex)  \n",
    "            elif 'Connection error' in str(ex):  \n",
    "                print('Error: Connection error')  \n",
    "                return 'Error: Connection error'  \n",
    "            elif str(ex.code) == \"content_filter\":  \n",
    "                print('Content Filter Error', ex.code)  \n",
    "                return \"Error: Content could not be extracted due to Azure OpenAI content filter.\" + ex.code  \n",
    "            else:  \n",
    "                print('API Error:', ex)  \n",
    "                print('API Error Code:', ex.code)  \n",
    "                incremental_backoff = min(max_backoff, incremental_backoff * 1.5)  \n",
    "                counter += 1  \n",
    "                time.sleep(incremental_backoff)  \n",
    "        except Exception as ex:  \n",
    "            counter += 1  \n",
    "            print('Error - Retry count:', counter, ex)  \n",
    "            return \"\"  \n",
    "  \n",
    "# Function to generate SQL query using OpenAI  \n",
    "def generate_query(question, schema_details, previous_response, openai_gpt_api_base, openai_gpt_api_key, openai_gpt_api_version, openai_gpt_model):  \n",
    "    max_attempts = 6  \n",
    "    max_backoff = 60  \n",
    "    system_prompt = \"\"\"  \n",
    "    You are an intelligent assistant that helps create a SQL Server query that retrieves data relating to a user's question.  \n",
    "    You 'may' be provided with some \"additional_data\" which may help you to create this query.  \n",
    "    Ensure your queries use both the owner and table names (for example dbo.Sales).  \n",
    "    If you need more details from the user to create a valid query, please ask for these details, otherwise only return the SQL Query in your response.  \n",
    "    Use only the schema details provided by the user and NEVER make up or guess table or column names.  \n",
    "    For filtering using WHERE statements, never guess or assume the value to use for the WHERE value.  \n",
    "    For example, if the question asks to show the data where state is WA. Do not assume the filter value is \"WA\".  \n",
    "    If you need more information, please request it.  \n",
    "    Unless you need more details, always respond with a SQL Query only as your response.  \n",
    "    Before sending back a query, make sure everything needed to execute the query exists in the schema provided by the user.  \n",
    "    \"\"\"  \n",
    "    user_prompt = \"Please help me create a SQL Server query that retrieves data to answer this question: \" + question  \n",
    "    if previous_response:  \n",
    "        user_prompt += 'additional_data: ' + previous_response + '\\n\\n'  \n",
    "    user_prompt += \"Here are the schema details for this database:\\n\" + schema_details  \n",
    "  \n",
    "    gpt_client = AzureOpenAI(  \n",
    "        api_version=openai_gpt_api_version,  \n",
    "        azure_endpoint=openai_gpt_api_base,  \n",
    "        api_key=openai_gpt_api_key  \n",
    "    )  \n",
    "  \n",
    "    counter = 0  \n",
    "    incremental_backoff = 1  # seconds to wait on throttling - this will be incremental backoff  \n",
    "    while counter < max_attempts:  \n",
    "        try:  \n",
    "            response = gpt_client.chat.completions.create(  \n",
    "                model=openai_gpt_model,  \n",
    "                messages=[  \n",
    "                    {\"role\": \"system\", \"content\": system_prompt},  \n",
    "                    {\"role\": \"user\", \"content\": user_prompt}  \n",
    "                ],  \n",
    "                temperature=openai_temperature,  \n",
    "                max_tokens=max_tokens,  \n",
    "                top_p=0.95,  \n",
    "                frequency_penalty=0,  \n",
    "                presence_penalty=0,  \n",
    "                stop=None,  \n",
    "                stream=False  \n",
    "            )  \n",
    "            return response.choices[0].message.content  \n",
    "        except openai.APIError as ex:  \n",
    "            if str(ex.code) == \"429\":  \n",
    "                incremental_backoff = min(max_backoff, incremental_backoff * 1.5)  \n",
    "                print('Waiting to retry after', incremental_backoff, 'seconds...')  \n",
    "                counter += 1  \n",
    "                time.sleep(incremental_backoff)  \n",
    "            elif str(ex.code) == \"DeploymentNotFound\":  \n",
    "                print('Error: Deployment not found')  \n",
    "                return 'Error: Deployment not found'  \n",
    "            elif 'Error code: 40' in str(ex):  \n",
    "                print('Error: ' + str(ex))  \n",
    "                return 'Error:' + str(ex)  \n",
    "            elif 'Connection error' in str(ex):  \n",
    "                print('Error: Connection error')  \n",
    "                return 'Error: Connection error'  \n",
    "            elif str(ex.code) == \"content_filter\":  \n",
    "                print('Content Filter Error', ex.code)  \n",
    "                return \"Error: Content could not be extracted due to Azure OpenAI content filter.\" + ex.code  \n",
    "            else:  \n",
    "                print('API Error:', ex)  \n",
    "                print('API Error Code:', ex.code)  \n",
    "                incremental_backoff = min(max_backoff, incremental_backoff * 1.5)  \n",
    "                counter += 1  \n",
    "                time.sleep(incremental_backoff)  \n",
    "        except Exception as ex:  \n",
    "            counter += 1  \n",
    "            print('Error - Retry count:', counter, ex)  \n",
    "            return \"\"  \n",
    "  \n",
    "# Function to generate text answer using OpenAI  \n",
    "def generate_text_answer(question, sql_query_response, openai_gpt_api_base, openai_gpt_api_key, openai_gpt_api_version, openai_gpt_model):  \n",
    "    max_attempts = 6  \n",
    "    max_backoff = 60  \n",
    "    system_prompt = \"\"\"  \n",
    "    You are an intelligent assistant that helps take a user's question and a markdown table and tries to create a textual response to the user's question based on the data from the markdown table.  \n",
    "    You can assume that the data included does answer the question.  \n",
    "    Only use the information in the table to try to answer the question and do not make anything up.  \n",
    "    \"\"\"  \n",
    "    user_prompt = 'Question: ' + question + '\\n'  \n",
    "    user_prompt += 'Answer: ' + sql_query_response + '\\n'  \n",
    "    gpt_client = AzureOpenAI(  \n",
    "        api_version=openai_gpt_api_version,  \n",
    "        azure_endpoint=openai_gpt_api_base,  \n",
    "        api_key=openai_gpt_api_key  \n",
    "    )  \n",
    "  \n",
    "    counter = 0  \n",
    "    incremental_backoff = 1  # seconds to wait on throttling - this will be incremental backoff  \n",
    "    while counter < max_attempts:  \n",
    "        try:  \n",
    "            response = gpt_client.chat.completions.create(  \n",
    "                model=openai_gpt_model,  \n",
    "                messages=[  \n",
    "                    {\"role\": \"system\", \"content\": system_prompt},  \n",
    "                    {\"role\": \"user\", \"content\": user_prompt}  \n",
    "                ],  \n",
    "                temperature=openai_temperature,  \n",
    "                max_tokens=max_tokens,  \n",
    "                top_p=0.95,  \n",
    "                frequency_penalty=0,  \n",
    "                presence_penalty=0,  \n",
    "                stop=None,  \n",
    "                stream=False  \n",
    "            )  \n",
    "            return response.choices[0].message.content  \n",
    "        except openai.APIError as ex:  \n",
    "            if str(ex.code) == \"429\":  \n",
    "                incremental_backoff = min(max_backoff, incremental_backoff * 1.5)  \n",
    "                print('Waiting to retry after', incremental_backoff, 'seconds...')  \n",
    "                counter += 1  \n",
    "                time.sleep(incremental_backoff)  \n",
    "            elif str(ex.code) == \"DeploymentNotFound\":  \n",
    "                print('Error: Deployment not found')  \n",
    "                return 'Error: Deployment not found'  \n",
    "            elif 'Error code: 40' in str(ex):  \n",
    "                print('Error: ' + str(ex))  \n",
    "                return 'Error:' + str(ex)  \n",
    "            elif 'Connection error' in str(ex):  \n",
    "                print('Error: Connection error')  \n",
    "                return 'Error: Connection error'  \n",
    "            elif str(ex.code) == \"content_filter\":  \n",
    "                print('Content Filter Error', ex.code)  \n",
    "                return \"Error: Content could not be extracted due to Azure OpenAI content filter.\" + ex.code  \n",
    "            else:  \n",
    "                print('API Error:', ex)  \n",
    "                print('API Error Code:', ex.code)  \n",
    "                incremental_backoff = min(max_backoff, incremental_backoff * 1.5)  \n",
    "                counter += 1  \n",
    "                time.sleep(incremental_backoff)  \n",
    "        except Exception as ex:  \n",
    "            counter += 1  \n",
    "            print('Error - Retry count:', counter, ex)  \n",
    "            return \"\"  \n",
    "  \n",
    "# Function to generate vectors for text  \n",
    "def generate_embedding(text, openai_embedding_api_version, openai_embedding_api_base, openai_embedding_api_key, openai_embeddings_model):  \n",
    "    max_attempts = 6  \n",
    "    max_backoff = 60  \n",
    "    if text is None:  \n",
    "        return None  \n",
    "  \n",
    "    client = AzureOpenAI(  \n",
    "        api_version=openai_embedding_api_version,  \n",
    "        azure_endpoint=openai_embedding_api_base,  \n",
    "        api_key=openai_embedding_api_key  \n",
    "    )  \n",
    "  \n",
    "    counter = 0  \n",
    "    incremental_backoff = 1  # seconds to wait on throttling - this will be incremental backoff  \n",
    "    while counter < max_attempts:  \n",
    "        try:  \n",
    "            response = client.embeddings.create(  \n",
    "                input=text,  \n",
    "                model=openai_embeddings_model  \n",
    "            )  \n",
    "            return json.loads(response.model_dump_json())[\"data\"][0]['embedding']  \n",
    "        except OpenAIError as ex:  \n",
    "            if str(ex.code) == \"429\":  \n",
    "                print('OpenAI Throttling Error - Waiting to retry after', incremental_backoff, 'seconds...')  \n",
    "                incremental_backoff = min(max_backoff, incremental_backoff * 1.5)  \n",
    "                counter += 1  \n",
    "                time.sleep(incremental_backoff)  \n",
    "            elif str(ex.code) == \"DeploymentNotFound\":  \n",
    "                print('Error: Deployment not found')  \n",
    "                return 'Error: Deployment not found'  \n",
    "            elif 'Error code: 40' in str(ex):  \n",
    "                print('Error: ' + str(ex))  \n",
    "                return 'Error:' + str(ex)  \n",
    "            elif 'Connection error' in str(ex):  \n",
    "                print('Error: Connection error')  \n",
    "                return 'Error: Connection error'  \n",
    "            elif str(ex.code) == \"content_filter\":  \n",
    "                print('Content Filter Error', ex.code)  \n",
    "                return \"Error: Content could not be extracted due to Azure OpenAI content filter.\" + ex.code  \n",
    "            else:  \n",
    "                print('API Error:', ex)  \n",
    "                print('API Error Code:', ex.code)  \n",
    "                incremental_backoff = min(max_backoff, incremental_backoff * 1.5)  \n",
    "                counter += 1  \n",
    "                time.sleep(incremental_backoff)  \n",
    "        except Exception as ex:  \n",
    "            counter += 1  \n",
    "            print('Error - Retry count:', counter, ex)  \n",
    "            return None  \n",
    "  \n",
    "# Function to get SQL Server schema information  \n",
    "def get_sql_server_schema_info(server, database, username, password):  \n",
    "    conn = pyodbc.connect(  \n",
    "        f'DRIVER={{ODBC Driver 18 for SQL Server}};'  \n",
    "        f'SERVER={server};'  \n",
    "        f'DATABASE={database};'  \n",
    "        f'UID={username};'  \n",
    "        f'PWD={password}'  \n",
    "    )  \n",
    "    cursor = conn.cursor()  \n",
    "  \n",
    "    # Fetch all table names  \n",
    "    cursor.execute(\"SELECT TABLE_SCHEMA, TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE='BASE TABLE';\")  \n",
    "    tables = cursor.fetchall()  \n",
    "  \n",
    "    schema_info = {}  \n",
    "    for table in tables:  \n",
    "        table_owner = table[0]  \n",
    "        table_name = table[1]  \n",
    "  \n",
    "        # Fetch column details  \n",
    "        cursor.execute(f\"SELECT COLUMN_NAME, DATA_TYPE FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME='{table_name}';\")  \n",
    "        columns = cursor.fetchall()  \n",
    "  \n",
    "        # Fetch primary key details  \n",
    "        cursor.execute(f\"\"\"  \n",
    "            SELECT COLUMN_NAME  \n",
    "            FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS TC  \n",
    "            JOIN INFORMATION_SCHEMA.CONSTRAINT_COLUMN_USAGE CCU   \n",
    "            ON TC.CONSTRAINT_NAME = CCU.CONSTRAINT_NAME  \n",
    "            WHERE TC.TABLE_NAME = '{table_name}' AND TC.CONSTRAINT_TYPE = 'PRIMARY KEY';  \n",
    "        \"\"\")  \n",
    "        primary_keys = cursor.fetchall()  \n",
    "        primary_keys = [pk[0] for pk in primary_keys]  \n",
    "  \n",
    "        # Fetch foreign key details  \n",
    "        cursor.execute(f\"\"\"  \n",
    "            SELECT   \n",
    "                KCU.COLUMN_NAME,  \n",
    "                KCU.CONSTRAINT_NAME,  \n",
    "                RC.UPDATE_RULE,  \n",
    "                RC.DELETE_RULE,  \n",
    "                KCU2.TABLE_NAME AS REFERENCED_TABLE_NAME,  \n",
    "                KCU2.COLUMN_NAME AS REFERENCED_COLUMN_NAME  \n",
    "            FROM   \n",
    "                INFORMATION_SCHEMA.REFERENTIAL_CONSTRAINTS RC  \n",
    "                JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE KCU  \n",
    "                ON KCU.CONSTRAINT_NAME = RC.CONSTRAINT_NAME  \n",
    "                JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE KCU2  \n",
    "                ON KCU2.CONSTRAINT_NAME = RC.UNIQUE_CONSTRAINT_NAME  \n",
    "            WHERE KCU.TABLE_NAME = '{table_name}';  \n",
    "        \"\"\")  \n",
    "        foreign_keys = cursor.fetchall()  \n",
    "  \n",
    "        column_info = []  \n",
    "        for col in columns:  \n",
    "            column_name = col[0]  \n",
    "            column_type = col[1]  \n",
    "            description = f\"{column_name} ({column_type})\"  \n",
    "            if column_name in primary_keys:  \n",
    "                description += \" [Primary Key]\"  \n",
    "            for fk in foreign_keys:  \n",
    "                if column_name == fk[0]:  \n",
    "                    description += f\" [Foreign Key to {fk[4]}({fk[5]})]\"  \n",
    "            column_info.append((column_name, column_type, description))  \n",
    "  \n",
    "        # Fetch sample rows  \n",
    "        markdown_table = ''  \n",
    "        try:  \n",
    "            cursor.execute(f\"SELECT TOP 40 * FROM {table_owner}.{table_name}\")  \n",
    "            rows = cursor.fetchall()  \n",
    "  \n",
    "            # Randomly select 5-10 rows for diversity  \n",
    "            selected_rows = random.sample(rows, min(len(rows), 3))  \n",
    "  \n",
    "            # Convert sample rows to markdown table format  \n",
    "            if selected_rows:  \n",
    "                headers = [desc[0] for desc in cursor.description]  \n",
    "                markdown_table += \"\\n| \" + \" | \".join(headers) + \" |\\n\"  \n",
    "                markdown_table += \"| \" + \" | \".join(['---'] * len(headers)) + \" |\\n\"  \n",
    "                for row in selected_rows:  \n",
    "                    markdown_table += \"| \" + \" | \".join([str(cell) if cell is not None else 'NULL' for cell in row]) + \" |\\n\"  \n",
    "  \n",
    "        except pyodbc.ProgrammingError as e:  \n",
    "            if 'ODBC SQL type -151' in str(e):  \n",
    "                markdown_table += \"\\n| Error | This table contains unsupported data types and cannot be sampled |\\n\"  \n",
    "            else:  \n",
    "                raise e  \n",
    "  \n",
    "        column_info.append(('Sample Data from Table', markdown_table, ''))  \n",
    "        schema_info[table_owner + '.' + table_name] = column_info  \n",
    "  \n",
    "    conn.close()  \n",
    "    return schema_info  \n",
    "  \n",
    "# Function to execute SQL query  \n",
    "def execute_query(query):  \n",
    "    conn = pyodbc.connect(  \n",
    "        f'DRIVER={{ODBC Driver 18 for SQL Server}};'  \n",
    "        f'SERVER={server};'  \n",
    "        f'DATABASE={database};'  \n",
    "        f'UID={username};'  \n",
    "        f'PWD={password}'  \n",
    "    )  \n",
    "  \n",
    "    df = pd.read_sql(query, conn)  \n",
    "  \n",
    "    # Convert dataframe to Markdown table  \n",
    "    markdown_table = df.to_markdown(index=False)  \n",
    "  \n",
    "    return markdown_table  \n",
    "\n",
    "# Function to generate table description  \n",
    "def generate_table_description(table_name, columns):  \n",
    "    schema_details = f\"Table Name: {table_name}\\nColumns:\\n\"  \n",
    "    for column_name, column_type, description in columns:  \n",
    "        if column_name == 'Sample Data from Table':  \n",
    "            schema_details += f\"- {column_name} \\n {column_type}\\n\"  \n",
    "        else:  \n",
    "            schema_details += f\"- {column_name} ({column_type}): {description}\\n\"  \n",
    "    return generate_oai_table_desc(schema_details, openai_gpt_api_base, openai_gpt_api_key, openai_gpt_api_version, openai_gpt_model)  \n",
    "  \n",
    "# Function to vectorize descriptions  \n",
    "def vectorize_descriptions(table_descriptions):  \n",
    "    description_vectors = {}  \n",
    "    for table_name, description in table_descriptions.items():  \n",
    "        description_vectors[table_name] = generate_embedding(description, openai_embedding_api_version, openai_embedding_api_base, openai_embedding_api_key, openai_embedding_model)  \n",
    "    return description_vectors  \n",
    "\n",
    "# Function to execute query request  \n",
    "def execute_query_request(question, merged_table_descriptions_markdown):  \n",
    "    sql_query_response = generate_query(question, merged_table_descriptions_markdown, None, openai_gpt_api_base, openai_gpt_api_key, openai_gpt_api_version, openai_gpt_model)  \n",
    "    if '```sql' in sql_query_response:  \n",
    "        sql_query_response = sql_query_response[sql_query_response.find('```sql') + 6:]  \n",
    "    if '```' in sql_query_response:  \n",
    "        sql_query_response = sql_query_response[:sql_query_response.find('```')]  \n",
    "      \n",
    "    print(sql_query_response)  \n",
    "      \n",
    "    response = execute_query(sql_query_response)  \n",
    "    answer = generate_text_answer(question, response, openai_gpt_api_base, openai_gpt_api_key, openai_gpt_api_version, openai_gpt_model)  \n",
    "      \n",
    "    display(Markdown('## ' + question + '\\n\\n' + response))  \n",
    "    print('\\n --------------------------------------- \\n')  \n",
    "    print('Textual Answer')  \n",
    "    print(answer)  \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6da99e3a-1e1f-4e9e-baa9-ee8d6c008b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving SQL Schema information...\n",
      "Creating table descriptions...\n",
      "Processing table: dbo.InventoryItems ...\n",
      "Processing table: dbo.CustomerAccounts ...\n",
      "Processing table: dbo.BankInformation ...\n",
      "Processing table: dbo.ExpenseClaimAudit ...\n",
      "Processing table: dbo.BudgetAllocationAudit ...\n",
      "Processing table: dbo.TaxInformation ...\n",
      "Processing table: dbo.PortfolioInvestments ...\n",
      "Processing table: dbo.FinancialStatementAudit ...\n",
      "Processing table: dbo.FinancialRiskAudit ...\n",
      "Processing table: dbo.FinancialAuditMaster ...\n",
      "Processing table: dbo.EmployeeInfo ...\n",
      "Processing table: dbo.LoanRepayments ...\n",
      "Processing table: dbo.sysdiagrams ...\n",
      "Processing table: dbo.ProjectTasks ...\n",
      "Processing table: dbo.VendorDetails ...\n",
      "Merging table descriptions into markdown...\n",
      "Markdown length: 37722\n"
     ]
    }
   ],
   "source": [
    "# Generate schema information  \n",
    "print ('Retrieving SQL Schema information...')\n",
    "schema_info = get_sql_server_schema_info(server, database, username, password)  \n",
    "  \n",
    "# Generate descriptions for all tables  \n",
    "print ('Creating table descriptions...')\n",
    "table_descriptions = {}  \n",
    "for table_name, columns in schema_info.items():  \n",
    "    print ('Processing table:', table_name)\n",
    "    description = generate_table_description(table_name, columns)  \n",
    "    table_descriptions[table_name] = description  \n",
    "  \n",
    "# Save table descriptions to a file  \n",
    "with open('table_descriptions.pkl', 'wb') as pkl_out:  \n",
    "    pickle.dump(table_descriptions, pkl_out)  \n",
    "  \n",
    "# # Vectorize descriptions  \n",
    "# print ('Vectorizing table details...')\n",
    "# description_vectors = vectorize_descriptions(table_descriptions)  \n",
    "  \n",
    "# # Save description vectors to a file  \n",
    "# with open('description_vectors.pkl', 'wb') as pkl_out:  \n",
    "#     pickle.dump(description_vectors, pkl_out)  \n",
    "  \n",
    "# Merge table descriptions into a single markdown string  \n",
    "print ('Merging table descriptions into markdown...')\n",
    "merged_table_descriptions_markdown = ''  \n",
    "for td in table_descriptions:  \n",
    "    merged_table_descriptions_markdown += table_descriptions[td] + '\\n\\n'  \n",
    "  \n",
    "# Save the merged table descriptions to a text file  \n",
    "with open('merged_table_descriptions_markdown.txt', 'w') as f_out:  \n",
    "    f_out.write(merged_table_descriptions_markdown)  \n",
    "  \n",
    "print('Markdown length:', len(merged_table_descriptions_markdown))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be4f8dd3-040a-427f-bb9b-17bbed5951d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the merged table descriptions from the text file  \n",
    "with open('merged_table_descriptions_markdown.txt', 'r') as f_in:  \n",
    "    merged_table_descriptions_markdown = f_in.read()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76467d1c-5d58-4074-9849-b61e83106f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT TOP 1\n",
      "    LoanType,\n",
      "    COUNT(*) AS LoanTypeCount\n",
      "FROM\n",
      "    dbo.LoanRepayments\n",
      "GROUP BY\n",
      "    LoanType\n",
      "ORDER BY\n",
      "    LoanTypeCount DESC;\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_179/242020169.py:362: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## What is the most common loan type customers have?\n",
       "\n",
       "| LoanType              |   LoanTypeCount |\n",
       "|:----------------------|----------------:|\n",
       "| Audit Compliance Loan |            7958 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " --------------------------------------- \n",
      "\n",
      "Textual Answer\n",
      "The most common loan type customers have is the Audit Compliance Loan, with a count of 7,958.\n"
     ]
    }
   ],
   "source": [
    "# Example usage  \n",
    "question = \"What is the most common loan type customers have?\"  \n",
    "execute_query_request(question, merged_table_descriptions_markdown)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9e2e66-f736-43cc-89da-b2930355723a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
